# Fall-2024-Undergrad-Research

This repository contains all of the key work from my Fall 2024 undergrad research project, where I worked with the Indiana University computer graphics courses and instructor Mitja Hmeljak.
The goal of this project was to develop a curriculum to teach AR graphics capabilities at IU, as well as integrate AR into the existing graphics courses, using Unity and it's AR Foundation package. 
The work is broken down by week, with the completed code base and guides I made stored separately. Any missing week just means school was on break or the class part of research took precedent that week, rather than the actual research project itself.

### Philosophy
This is my personal philosophy for taking on this project and working through it, and does not reflect those of anyone else involved. There are 2 main things that motivated me for this project. First, AR can be a powerful tool when teaching students 3D environments and graphics. Navigating a 3D environment with 2D inputs can be cumbersome, and using AR allows students to use their intuitive understanding of the real world and motion to better understand the environment. Motion in 2D is also limited to moment along 1 axis or 1 plane, but brining motion into AR allows for true 3D movement. For a new student, objects being 'over here' and 'over there' could be more helpful than knowing an object is a (1,1,0) and (5,0,5). By This is especially true of lights, where in Unity the are represented as invisible points. Its easy to get lost in positioning when endlessly rotating and scrolling, but in the real world, we can see the light, the object, and move around it in an easy to understand way, seeing the whole scene and positioning at once. If we can develop an easy way to onboard students on to AR, further graphical work can be done with a more intuitive understanding of the environment. We can also use this to create an introduction into AR graphics and further AR work, as shown with the guides we created.

The second goal that motivated me in this project was learning AR and app development in a way that meaningfully took advantage of the AR environment. There are many apps out there that use AR, but basically act as some bit of data overlayed onto a camera feed, with no real interaction with the real world. Sure, it would be cool to see a picture someone posted floating in space or an arrow floating in front of me showing which way to go, but all this amounts to is Instagram or a map app that is more inconvenient to use. During this project, I wanted to make sure any AR application truly had reason to be in AR, and not just a 3D object you had to hold your phone a certain way to see. For the most part I held true to this, the exception being the acceleration demo which is just a bar graph on top of a camera feed, but for all intents and purposes we can just say that was to may homage to early pre-AR Kit applications.

### Week 4
To start I made a simple plane detection test and a custom collision script that allows the user to use a 3D cursor to 'tap' walls or floors and test collisions. I believe for this first demo I made it so only the first plane detected was stored and used for collisions, as well as only detected floors so I could focus on testing one axis at a time. This demo left a lot to be desired and was heavily improved over time. Even with a year of Unity under my belt at this point, this was my first adventure into AR and some of the maths we would use, and it definitely shows.

### Week 5
Fixed up demo 1 a little bit to work on planes. I also created a second demo to test 'in object detection'. This demo allows the user to spawn a cube made out of the same data structure AR Foundation uses for surface meshes. This was actually significantly easier than testing for collisions with a surface, since a surface requires some complex dot products to test for the 1D bounds of a mesh, but testing for the 2D plane bounds of an object simply requires checking the signs of the distance to each plane. If all distances are negative, the point is within the object. This fails on very complex or concave objects, but for our simple tests of data input and digital/real world environment meshing I decided to move onto the next demos and return to this if time allowed (it did not).

### Week 6
Unity provides a collection of demos for AR Foundation, and one of these demos includes a 'Debug Plane' prefab which shows which way the normal of the surface is facing, as well as its classifications. I added this to the surface detection demo to better understand how Unity saw the world. I also turned the in object demo into an 'in room' demo, which is the exact same as the in object demo, but passes if all distances are positive instead of negative. I also did some testing with getting acceleration data from the phone, which is not a part of AR Foundation but just a direct input from Unity's input system, but due to the way I tried to represent the data this was a noisy mess and was fully revamped in a later week.

### Week 7
Changed the acceleration demo to be more clear. This demo shows total acceleration, linear acceleration (acceleration without the effect of gravity), and acceleration due to gravity. This was a pretty simple week, so I also made 3 additional demos purely out of my own interest. Demo X is a simple surface detector, but applies a texture to the detected surfaces. Demo Y creates a floor plan out of a scanned room and exports it in the UDFM format, which means you can 3D scan a room and load it into the 1994 video game Doom II. As far as I'm concerned, this is the peak of computer science and honestly we might as well turn off all computers at this point. Demo Z, which was later removed due to compiling issues, tested AR Foundation's 2D image tracking by attaching simple models to detected April Tags.

